# -*- coding: utf-8 -*-
"""Cleaning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19v6P9eKhPlNnyN83_-Ydv-By5YKXD0Nx
"""

#Import libraries/modules
from google.cloud import storage
import pandas as pd
from io import StringIO


#Create a client object that points to GCS
storage_client = storage.Client()


#Point to bucket
source_bucket_name="my-bigdata-project-mh"
bucket=storage_client.get_bucket(source_bucket_name)


# Get a list of the 'blobs' (objects or files) in the landing folder of the source bucket
blobs = storage_client.list_blobs(source_bucket_name, prefix="landing/")


#Make list
filtered_blobs = [blob for blob in blobs if blob.name.endswith('.csv')]


#Go through each file and perform cleaning
for blob in filtered_blobs:
    print(f"Processing file {blob.name} with size {blob.size} bytes")
    source_file_path=f"gs://{source_bucket_name}/landing/{blob.name}"
    df = pd.read_csv(StringIO(blob.download_as_text()), header=0, sep=",")


    #Remove rows with missing values
    df= df.dropna()


    #Add ticker column
    filename = blob.name.replace('landing/', '')
    filename_parts = filename.split('_')
    ticker_symbol = filename_parts[0]
    df['ticker_symbol'] = ticker_symbol


    #Save the Pandas dataframe to a parquet file and store in a buffer called filedata
    filedata = df.to_parquet(index=False)


    #Create a blob with the associated file name
    #Source code:https://stackoverflow.com/questions/47141291/upload-file-to-google-cloud-storage-bucket-sub-directory-using-pythons (for cleaned folder path)
     blob2=bucket.blob(f"cleaned/{ticker_symbol}_minute_data_with_indicators.parquet")


    #Upload the filedata buffer to the file blob in the GCS bucket
    blob2.upload_from_string(filedata, content_type='application/octet-stream')